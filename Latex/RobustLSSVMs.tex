
\documentclass[preprint,12pt]{elsarticle}
%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% The graphicx package provides the includegraphics command.
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath,empheq, amsfonts}
\usepackage{subcaption}

\usepackage{amsthm}
\newtheorem*{remark}{Remark}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
\usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

\journal{Neurocomputing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

\title{Sparse Robust Least Squares Support Vector Machines for Classification}

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}


%% use optional labels to link authors explicitly to addresses:


\author[statistics]{Iwein Vranckx\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead[url]{wis.kuleuven.be/stat/robust}
\ead[mail]{iwein.vranckx@kuleuven.be}

\author[stadius]{Joachim Schreurs}
\author[mebios]{Bart De Ketelaere}
\author[statistics]{Mia Hubert}
\author[stadius]{Johan Suykens}
%
\address[statistics]{KU Leuven, Department of Mathematics and LStat, Celestijnenlaan 200B, BE-3001 Heverlee, Belgium}
\address[stadius]{KU Leuven, ESAT-STADIUS, Kasteelpark Arenberg 10, BE-3001 Heverlee, Belgium}
\address[mebios]{KU Leuven, Division of Mechatronics, Biostatistics and Sensors, Kasteelpark Arenberg 30, BE-3001 Heverlee, Belgium}

\begin{abstract}
(Abstract uit weighted LS-SVM, ter voorbeeld). Least squares support vector machines (LS-SVM) is an SVM version which involves
equality instead of inequality constraints and works with a least squares cost function.
In this way, the solution follows from a linear Karush–Kuhn–Tucker system instead of
a quadratic programming problem. However, sparseness is lost in the LS-SVM case and
the estimation of the support values is only optimal in the case of a Gaussian distribution
of the error variables. In this paper, we discuss a method which can overcome these two
drawbacks. We show how to obtain robust estimates for regression by applying a weighted
version of LS-SVM. We also discuss a sparse approximation procedure for weighted and
unweighted LS-SVM. It is basically a pruning method which is able to do pruning based
upon the physical meaning of the sorted support values, while pruning procedures for
classical multilayer perceptrons require the computation of a Hessian matrix or its inverse.
The methods of this paper are illustrated for RBF kernels and demonstrate how to obtain
robust estimates with selection of an appropriate number of hidden units, in the case of
outliers or non-Gaussian error distributions with heavy tails

\end{abstract}

\begin{keyword}
	Robust support vector machines\sep Non-linear outlier detection \sep Support vector pruning \sep Sparse LS-SVM 
\end{keyword}
\end{frontmatter}

%\linenumbers

%% main text
\newpage
\section{Introduction}

The least-squares support vector machines (LS-SVM) is powerful method for solving pattern recognition and regression problems. The LS-SVM maps the data to a higher dimensional space in which one constructs an optimal separating hyperplane. It has been applied to many real-world problems such as optimal control~\cite{suykens2001optimal}, financial time series prediction~\cite{van2001financial}, system identification~\cite{goethals2005identification}, electrical load forecasting~\cite{espinoza2006fixed} and may others. However the LS-SVM has two main disadvantages. It is sensitive to outliers which have large support values which result from the solution to the linear system. Where the LS-SVM support values are proportional to the errors at the datapoints. The second disadvantage is that the solution lacks sparseness, which is essential for real-time prediction with big-data.

To reduce the influence of outliers, Suykens et al. proposed the weighted LS-SVM~\cite{suykens2002weighted}. By iteratively assigning small weights to outliers and retraining, the method diminishes the effect of extreme values. Other solution where proposed by Yant et. al~\cite{yang2014robust}, where a truncated loss function is used in the objective function. Which is solved by a concave-convex procedure and the newton algorithm. And robustified least squares support vector classification~\cite{debruyne2009robustified}, which proposed a weighted LS-SVM classification where weights are equal to spatial rank with respect to the other feature vectors in the group.

In comparison to the standard Support Vector Machines (SVM) it only requires solving a linear system, but it lacks sparseness in the number of solution terms. To solve this problem, Suykens et. al~\cite{suykens2000sparse} proposes a method that iteratively prunes the datapoints with the lowest support value and retrains. Yang et.al propose a one-step compressive pruning strategy to reduce the number of support vectors~\cite{yang2014sparse}. Fixed-size LS-SVM sparsifies the LS-SVM by selecting important point or landmark points based on the quadratic Renyi Entropy criterion~\cite{suykens2002least}. However the landmark points are fixed in advance and don't take into account information of the classification itself, which could lead to sub-optimal results. This is in contrast to the sparse LS-SVM~\cite{suykens2000sparse}, which chooses datapoints based on the impact on the classification boundary. A comparison of different pruning methods can be found in~\cite{hoegaerts2004comparison}. 

In this paper, we propose a method to solve the two problems at once.
Our mean contributions consist off:
\begin{enumerate}
	\item Weighted LS-SVM variant voor classificatie (new?).
	\item Kernel Concentration steps (new)
	\item Soft reweighting based on Stahel-Donoho outlyingness
	\item New Pruning stategy based on information transfer and Entropy (?)
\end{enumerate}
We are/should be robust against high degrees of contamination in non-linear classification problems. Other methods that try to tackle both problems at once are found in~\cite{chen2018sparse}, where a primal formulation of LS-SVM with a truncated loss is introduced, sparseness is obtained by the Nystrom approximation. A second method is the weighted LS-SVM of Suykens et. al~\cite{suykens2002weighted}. \\

The remainder of this paper is organized as follows. In section 2 we introduce our weighted least squares support vector machine (LS-SVM). In section 3 we propose our robust outlier detection routine, followed by our support vector sparseness routine (??). The extensive simulation results of both theoretical and real, industrial datasets listed in section 4 conform the robustness, prediction time speed-up and improved classifier efficiency of our proposed method. Finally, our main findings and suggestions for further research are summarized in the conclusion, section 5. 


\section{Weighted LS-SVM for classification}
\subsection{LS-SVM classification}
A binary classifier's goals is to learn a prediction model that assigns the correct label to an unseen test sample. Restated more formally, we seek an SVM-based classifier that fits an optimal hyperplane between the two classes, where the optimal hyperplane maximizes the distance to the closest point of either class. Maximizing the hyperplane margin $||w||^{-1}$, one obtains a classifier with good generalisation properties. More naturally formulated, this inherent principle leads to a (much) better classification performance on (unseen) test data, for example compared with density based estimators. \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
%%%%%%%%%%%%%
%%%%%%%%%%%%%	AANPASSEN AAN WEIGHTED LEAST SQUARES PRINCIPE

Given an $p$-variate trainingsset $x \in \mathbb{R}^p$ and the binary class label $y_i \in[+1,-1]$ for observation $x_i$ with index $i$, the following constrained optimization must be solved to obtain the WEIGHTED LS-SVM representation:

\begin{equation}
min \  J(w,b,e_i) = \frac{1}{2} w^T w + \frac{\gamma}{2} \sum_{i=1}^{n} e_i^2
\label{eq:costfunction}
\end{equation}
...subject to the equality constraints:
\begin{equation}
y_i[w^T \varphi(x_i) + b] = 1-e_i
\label{eq:lsconstraint}
\end{equation}

Here $w^T \varphi(x_i) + b$ is the hyperplane based decision function of the classifier with corresponding parameters $w$ and $b$. The weight vector $WEIGHT$ denotes the normalized outlier weight for each observation, $\gamma$ is used to control the over/under-fitting trade-off and $\varphi(.)$ is the transformation from input space $\mathbb{R}^p$ to the kernel feature space. \\

The specified constraint states that every given multivariate sample should be lie on the correct side of hyperplane. Stated differently, the classifier should predict the class label of each sample correctly, where each observation $x_i$ has an corresponding error $e_i$ due to the constraint equality sign. This, in turn, implies that a LS-SVM loses its support vector sparseness compared to ordinary SVM's. \\

This constrained optimization problem is solved by the optimality conditions of its Lagrangian as follows:
\begin{equation}
L(w,b,e;\alpha) = J(w,b,e) - \sum_{i=1}^{n} \alpha_i(y_i [ w^T \varphi(x_i) + b]-1 + e_i)
\label{eq:lagrangian}
\end{equation}
Here, $\alpha_i$ is the Lagrange multiplier whose value can be either positive or negative, which is also different from the standard SVM proposed by Vapnik. 
As a direct consequence of the equality sign in the given constraint the specified Lagrangian is now solvable trough a linear system of equations:
\begin{align}
\frac{\partial L}{\partial w} &= 0 \rightarrow w = \sum_{i=1}^{n} \alpha_i y_i \varphi(x_i) \\
\frac{\partial L}{\partial b} &= 0 \rightarrow \alpha^T y = 0 \\
\frac{\partial L}{\partial e} &= 0 \rightarrow \alpha = \gamma e \\
\frac{\partial L}{\partial \alpha_i} &= 0 \rightarrow y_i [w^T \varphi(x_i) + b ] = 1 - e_i 	
\end{align}
Combining the first and last equation and defining 
\begin{align}
\Omega_{ij} &= y_i y_j \varphi(x_i)^T \varphi(x_j) \\
&= y_i y_j K(x_i, x_j)
\end{align}
...yields the following set of linear equations.

\begin{equation}
	\begin{bmatrix}
		0_{(1,1)} & y_{(n,1)}^T \\
		y_{(n,1)} & \Omega_{(n,n)} + \gamma^{-1} I_{(n,n)} 
		\end{bmatrix}	
		\begin{bmatrix}
		b_{(1,1)} \\
		\alpha_{(n,1)}
		\end{bmatrix}
		=
		\begin{bmatrix}
		0_{(1,1)} \\
		\mathbf{1}_{(n,1)}
	\end{bmatrix}	
\end{equation}
The least squares solution of the aforementioned system of equations is then used to  obtain the Lagrange coefficients $\alpha$ and offset $b$, where the decision function is:
\begin{equation}
	\hat{y}(x) = \sum_{1}^{n} \alpha_i y_i K(x, x_j) + b	
	\label{eq:classification}
\end{equation}
Or, written in a more convenient matrix notation, short-writing $\beta_i= (\alpha_i \  y_i)^T$
\begin{equation}
	\hat{y}(x) = \mathbf{\beta} K + b \mathbf{1}
	\label{eq:prediction}
\end{equation}
$\hat{y}_i$ is the predicted output of the ith training data point. Note that $\alpha$ satisfies the linear constraint $\sum_{i=1}^{n} \alpha_i  e_i = 1$, and that the derivation for least squares support vector regression follows the same reasoning %\cite{Choi2009}.

\newpage
\section{(Proposed method) Robust classification by Kernelized minimum covariance determinant}

Uitleggen wat MCD is, waar het gebruikt wordt en dat dit het hart van ons algoritme is.
This concentration step uses the Mahalanobis distance between an observation and the center of the training data in that space as a outlying measure in order to detect outliers.\\

The Minimum Covariance Determinant (MCD) method (Rousseeuw, 1984) is a highly robust
estimator of multivariate location and scatter. Given an nxp data matrix X = (x1, . . . , xn)T with xi = (xi1, . . . , xip)T , its objective is to find h observations (with ceil(n + p + 1)/2) 6 h 6 n) whose covariance matrix has the lowest determinant. The MCD estimate of location $\mu$ is then the average of these h points, and the scatter estimate $\Sigma$ is a multiple of their covariance matrix. In addition to being highly resistant to outliers, the MCD is affine equivariant, i.e. the estimates behave properly under affine transformations of the data. Although the MCD was already introduced in 1984, its practical use only became feasible
since the introduction of the computationally efficient FASTMCD algorithm of Rousseeuw and
Van Driessen (1999). Since then the MCD has been applied in various fields such as quality
control, medicine, finance, image analysis and chemistry, see e.g. Hubert et al. (2008) and Hubert
and Debruyne (2010) for references. The MCD is also being used as a basis to develop
robust and computationally efficient multivariate techniques, such as e.g. principal component
analysis (Croux and Haesbroeck, 2000; Hubert et al., 2005), factor analysis (Pison et al., 2003),
classification (Hubert and Van Driessen, 2004), clustering (Hardin and Rocke, 2004), and multivariate regression (Rousseeuw et al., 2004). For a review see (Hubert et al., 2008).
In this article we will present a deterministic algorithm for robust location and scatter, coined Kernel-MCD (kMCD) \\

In this section we briefly describe the FASTMCD algorithm and the spatial median estimator, as our new algorithm will use aspects of both.

\subsection{Introducing the kernel minimum covariance determinant method}

A major component of the FASTMCD algorithm is the concentration step (C-step), which works
as follows. Given initial estimates $\mu_{old}$ for the center and $\Sigma_{old}$ for the scatter matrix,

\begin{enumerate}
	\item Compute the distances $d_{old(i)} = D(x_i, \mu_{old}, \Sigma_{old})$ for i = 1, . . . , n.
	\item Sort these distances
	\item compute $\mu_{new}, \Sigma_{new}$
	\item iterate....
\end{enumerate}

In Theorem 1 of Rousseeuw and Van Driessen (1999) it was proved that the determinants converge to a steady-state solution. Therefore, if we apply C-steps iteratively, the sequence of
determinants obtained in this way must converge in a finite number of steps. \\


The expectation of observations in kernel feature space is defined as:

\begin{equation}
\mathop{\mathbb{E}}[\phi(x)] = \int \phi(x) P(x) dx
\end{equation}

Where $P(x)$ denotes the probability distribution of the training samples. Since the
distribution $P(x)$ is usually unknown, one can estimate this expectation by the empirical
center of the training dataset, defined as:

\begin{equation}
	c_n = \frac{1}{n} \sum_{i=1}^{n} \phi(x_i)
\end{equation}
The Mahalanobis distance between a sample $\phi(x)$ and the empirical
center $c_n$ is defined as:
\begin{equation}
	d^2 = (\phi(x) - c_n) \Sigma^{-1} (\phi(x) - c_n)
\end{equation}
Here, $\Sigma$ is the biased covariance matrix of the observations in kernel feature space, formally defined as:
\begin{equation}
	\Sigma = \frac{1}{n} \sum_{i=1}^{n}	\big( (\phi(x_i) - c_n) (\phi(x_i) - c_n)^T \big)
	\label{eq:kernel_covariance}
\end{equation}
Without any explicit knowledge of the mapping function $\phi(.)$, the covariance matrix
cannot be expressed in terms of the data $\phi(x)$ in the feature space. To overcome this problem, we use the singular value decomposition of the covariance matrix $\Sigma = V^T D V$. $V$ denotes the matrix of eigenvectors $v_k$ of $\Sigma$ and $D$ is the matrix of corresponding singular values $\lambda_k$, for $k = 1, 2, ..., n$, where each $(\lambda_k, v_k)$ pair satisfies   
\begin{equation}
	\lambda_k v_k = \Sigma v_k
\end{equation}
From the definition of the covariance matrix, equation \ref{eq:kernel_covariance}, it can be seen that each eigenvector is a linear
combination of the samples $\phi(x_i)$ in the feature space:

%\paragraph{Algorithm 1---Kernel MCD based LS-SVM}





%\subsection{Kernel Minimum Covariance determinant}
%\subsection{Kernel Stahel-Donoho outlyingness}
%\subsection{Pruning of support vectors}

\paragraph{Algorithm 2--- Reweighted K-MCD based LS-SVM}

\newpage
\section{Imposing support vector sparseness}

\subsection{Selection of the best support vectors}

Having established the data model and optimized classification accuracy for uncontaminated datasets, let us return to the problem at hand: the development of a robust pruning strategy. This implies that prediction formula should be partitioned in a \textit{relevant} (the support vectors, found by the pruning algorithm) - and \textit{irrelevant} part.  An overview of existing pruning methods can be found in Hoegaerts et. al \cite{hoegaerts2004comparison}. A first approach was suggest by Suykens et. al \cite{suykens2000sparse}, where  sparseness is imposed by pruning support values from the sorted support value spectrum which results from the solution to the linear system.  The motivation comes from the fact that the LS-SVM support values are proportional to the errors at the datapoints. Thus omitting the points that contribute least to the training error. An example of the support value spectrum can be seen on Figure \ref{fig:BadAlpha}. The values with a high $|\alpha_k|$ reside close to the decision boundary and are thus important to classify correctly. However blindly taking the points with largest support value spectrum could lead to sub-optimal results. Firstly, when outliers are present, you want to be certain that these are not chosen. Secondly, points are chosen independently towards each other. This results in clumping of sv's. The first problem is address by running a kernelized minimum covariance determinant, which detects and omits potential outliers. The second problem is solved by introducing a "region of interest" (ROI), which consist of the points with the $\beta$ percentage highest absolute value $|\alpha_k|$. From the ROI, $h$ landmark points are determined, where $h$ is the desired amount of support vectors.

\begin{figure}[h]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\label{fig:BadAlpha}
		\caption{Example bad Alpha spectrum}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\label{fig:GoodPruning}
		\caption{Example good SV selection}
	\end{subfigure}
\end{figure}

\noindent
------- TODO -------- \\
Something about how it is important to first be outlier-free before doing landmark selection. When you want to maximize entropy or promote diversity, you will always select outliers.\\
---------------------

\subsubsection{Entropy}
Selection of the landmark points is based on quadratic Renyi entropy~\cite{girolami2002orthogonal} and the fixed size LS-SVM~\cite{suykens2002least}. The landmarks points are chosen to maximize the quadratic Renyi Entropy:
\begin{equation}
H_R = -\mathrm{log}\int p(x)^2 dx.
\end{equation}
The quadratic Renyi Entropy is approximated by the following equation\cite{girolami2002orthogonal}:
\begin{equation}
\int \hat{p}(x)^2dx = \frac{1}{N^2} 1_v^\mathrm{T}\Omega 1_v,
\end{equation}
where $1_v = [1;1;...;1]$ and a normalized kernel is assumed with respect to density estimation. In the fixed-size LS-SVM approach, one chooses a fixed working set of size $M$ which is initialized randomly. Afterwards, random points are swapped in and out. If the entropy increases, the swapped point is accepted, otherwise the original subset is kept. This process continues until the change in entropy is small or a fixed number of iterations is reached. 

In contrast to the original fixed-size LS-SVM formulation, which is used to find a representative subset for the Nystr\"{o}m approximation \cite{suykens2002least}. We propose to first determine to region of interest, which includes the points that have the highest contribution to the robust LS-SVM model. On this reduced dataset, the fixed-size LS-SVM algorithm is used to select the $h$ landmark points. This ensures that the ROI is well approximated and there is no clumping effect present. The improved sv selection for the toy-problem is visible on Figure \ref{fig:GoodPruning}.


\begin{remark}
	It is important to first omit outliers, before continuing with the entropy procedure. Large contributions to the entropy will come from elements that have little or no structure~\cite{girolami2002orthogonal}. NOG AANPASSEN
\end{remark}


\subsubsection{Determinantal point process}
Selection of landmark points is based on determinantal point processes (DPPs)~\cite{kulesza2012determinantal}. DPPs are particularly interesting for set selection problems where diversity is preferred. A point process $\mathcal{P}$ on a ground set $\mathcal{Y} = {1,2,...,N}$ is a probability measure over point patterns of $\mathcal{C}$ , which are finite subsets of $\mathcal{Y}$. When $\mathcal{C}$ is a random subset, drawn according to the DPP $\mathcal{P}$, we have:
\begin{equation}
\label{eq:origDPP}
	\mathcal{P}(C \subseteq \mathcal{Y}) = \mathrm{det}(K_{\mathcal{C}}),
\end{equation}
where $K \preceq I$ is a positive symmetric semidefinite matrix with all eigenvalues smaller then or equal to 1, containing the index elements of $\mathcal{Y}$. $K_{\mathcal{C}} = [K_{i,j}]_{i,j \in \mathcal{C}}$ contains the selected rows and columns of $K$ and $\mathrm{det}(K_{\emptyset}) = 1$. From equation \eqref{eq:origDPP} follows:
\begin{align}
	\mathcal{P}(i \in \mathcal{Y}) &= K_{i,i} \\
	\mathcal{P}(i,j \in \mathcal{Y}) &= K_{i,i}K_{j,j} - K_{i,j}K_{j,i}\\
	\label{eq:KDPP}
	&= \mathcal{P}(i \in \mathcal{Y})\mathcal{P}(j \in \mathcal{Y}) - K_{i,j}^2.
\end{align}
The diagonal elements of the kernel matrix give the marginal probability of inclusion for individual elements of $\mathcal{Y}$, whereas the off-diagonal elements determine the (anti-)correlation between pairs of elements. Thus for large values of $K_{i,j}$, or a high similarity, points are unlikely the appear together. 

In our case, we would like to build a DPP based on the kernel matrix $K$, which is done using L-ensembles~\cite{borodin2009determinantal}. The probability of observing a subset $C \subseteq \mathcal{Y}$ is now equal to:
\begin{equation}
	\mathrm{Pr}(\mathcal{C}) = \frac{\mathrm{det}(K_{\mathcal{C}})}{\mathrm{det}(K + I)},
\end{equation}
where $I$ is the identity matrix, and $K$ a positive semidefinite matrix  indexed by the elements of $\mathcal{Y}$. In contrast to equation \eqref{eq:origDPP}, $K$ only has to be positive semidefinite, while the eigenvalues previously where bounded above. When conditioning on a fixed cardinality $k = |\mathcal{C}|$, one gets the k-DPP~\cite{kulesza2011k}.

The landmark selection algorithm consists of the following steps:
We propose to first determine to region of interest, which includes the points that have the highest contribution to the robust LS-SVM model. On this reduced dataset, a $k$-DPP~\cite{kulesza2011k}, where $k = h$, is used to sample the $h$ landmark points. This ensures that the ROI is well approximated and there is no clumping effect present. The improved sv selection for the toy-problem is visible on Figure \ref{fig:GoodPruning}.

\begin{remark}
It is important to first omit outliers, before continuing with the DPP sampling. In order to promote diversity, points that have a high similarity have a low chance of being sampled together (see equation \eqref{eq:KDPP}). Consequently outliers, that have a low similarity to any other point in the dataset, have a high chance of being sampled.
\end{remark}



\subsection{Information transfer}
 The information contained in the prune candidates can then be transferred to the support vectors - an idea originally introduced in [???]. Starting from equation \ref{eq:prediction} with the appropriate matrix dimensions:
\begin{equation}
\hat{y}_{(1,n_2) } = \mathbf{\beta}_{(1,n_1)} K_{(n_1, n_2)} + b_{(1,1)} \ \mathbf{1}_{(1, n2)} 
\end{equation}
Introducing sparse matrices, we could partition this expression in a (non) support vector part, denoted by the subscript $S$ and $N$ respectively.
\begin{equation}
\hat{y}_{(1,n_2) } = \mathbf{\beta}_{S(1,n_1)} K_{S(n_1, n_2)} + \mathbf{\beta}_{N(1,n_1)} K_{N(n_1, n_2)} + b_{(1,1)} \ \mathbf{1}_{(1, n2)}
\end{equation}
Next, one transfers the information of $\mathbf{\beta}_{N} K_{N}$ using the update $\Delta\beta$: 
\begin{align}
\Delta \beta K_S &= \beta_N K_N  \\
\Delta \beta &= K^\dagger_S  \beta_N K_N
\end{align}
We now have obtained an explicit expression for the update of our Lagrange multipliers. 
\begin{align}
\hat{\beta}_S &= \beta_S + \Delta\beta
\end{align}
If we omit all zero rows in the matrices above we obtain a compressed matrix of size $m_1$ rows in $n_2$ dimensions. Here, $m_1$ equals the (a priori, before the training stage) defined number of support vectors. The classier prediction equation finally boils down to:
\begin{equation}
\hat{y}_{(1,n_2) } = \mathbf{\beta}_{(1,m)} K_{(m, n_2)} + b_{(1,1)} \ \mathbf{1}_{(1, n2)} 
\end{equation}
Which is implemented using equation \ref{eq:gpuprediction} given the knowledge that $n_1 = m$ - the number of a priori defined support vectors. \\

The only problem that remains is the section of the most relevant support vectors....

\paragraph{Algorithm 3--- Sparse K-MCD based LS-SVM}

summary all steps algorithm

\section{Experiments} 
\subsection{Simulation results} 

\subsubsection{Linear example}

Two Gaussians close, the reverse labels behind at large distance. See Robustified LS-SVM~\cite{debruyne2009robustified}

\subsubsection{Non-Linear example}

Yin-Yang, Two spiral dataset or Cross Dataset~\cite{yang2014robust}

\subsubsection{UCI}

Robust: Banana, Celveland heart, Glass, Heartstatlog, liver disorder, monk PIMA, ripley, Transfusion, Vehicle~\cite{yang2014robust}

Robust + sparse: Splice, Pendigits (choose two digits 3 vs 4), Satimage (1 vs 6), USPS (1 vs 2), Mushrooms, Protein (1 vs 2).\cite{chen2018sparse}
Outliers = 30 \% samples that where far away from decision hyperplane, then randomly sample 1/3 of them and flip labels. datasets are in https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ 

Robust: Wine dataset + Linear, Glass, Biscuit Dough, Alon colon cancer, Hepatocellular carcinoma dataset~\cite{debruyne2009robustified}. However mostly linear

\subsection{Industrial data results} 

\section{Conclusions and future work}

\section*{Acknowledgements}

We thank Johan Speybrouck for providing us the industrial datasets and Tim Wynants for his feedback throughout this project. We also acknowledge the financial support of the VLAIO, grant HBC.2016.0208, for making this industrial research possible.

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:

%\bibliographystyle{model1-num-names}
\bibliographystyle{plain}
\bibliography{sample}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use model1-num-names.bst.

%% References without bibTeX database:

%\begin{thebibliography}{00}
%
%%% \bibitem must have the following form:
%%%   \bibitem{key}...
%%%
%
%
%\end{thebibliography}


\end{document}

%%
%% End of file `elsarticle-template-1-num.tex'.